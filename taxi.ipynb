{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewmachnowski/artificial-intelligence-uw/blob/main/Taxi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs3crKMpZYiR"
      },
      "source": [
        "# Taxi\n",
        "\n",
        "Środowisko Taxi przypomina pracę taksówkarza w mieście. Złożone jest z kwadratowej planszy podzielonej na $n \\times n$ kwadratów jednostkowych, każdy z nich może być przejezdny bądź nie. Mamy wyróżnione cztery rogi, nazwane odpowiednio R, G, Y i B. Nasz agent pojawia się w losowym miejscu, a jego celem jest obsługa zlecenia przewiezienia klienta między zadanymi rogami. Powinien zrobić to jak najsprawniej. Przykładowa plansza wygląda następująco:\n",
        "\n",
        "![](env.png)\n",
        "<center>Agent znajduje się w żółtym polu i za zadanie ma przewieźć klienta z R (niebieskie) do B (fioletowe).</center>\n",
        "\n",
        "W każdym momencie, nasz agent będzie mógł wykonać jedną z sześciu akcji:\n",
        "<ol start=\"0\">\n",
        "  <li>Jedź w dół,</li>\n",
        "  <li>Jedź w górę,</li>\n",
        "  <li>Jedź w prawo,</li>\n",
        "  <li>Jedź w lewo,</li>\n",
        "  <li>Weź klienta,</li>\n",
        "  <li>Wysadź klienta.</li>\n",
        "</ol>\n",
        "\n",
        "Każda z akcji 0-3 daje nam zysk -1 (kosztuje 1), akcje 4-5 dają zysk 0, gdy są w wykonane w poprawnym momencie, i -10, gdy nie. Wykonanie zadania daje dużą nagrodę: 10n.\n",
        "\n",
        "\n",
        "## Zadanie:\n",
        "- zaimplementować Q-learning\n",
        "- dobrać parametry\n",
        "- sprawdzić, dla jakich rozmiarów planszy nasza metoda znajduje rozwiązanie (pośród 6, 10, 20 i 100)\n",
        "\n",
        "## Opis stanu gry:\n",
        "- stan gry jest reprezentowany przez jedną liczbę"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oekFVWFjZe0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999a5e3d-2f94-4f75-f8eb-c83eb2ffd0bb"
      },
      "source": [
        "try:\n",
        "    import gym\n",
        "except:\n",
        "    !apt-get install cmake swig\n",
        "    !pip install gym[all]\n",
        "\n",
        "data_url = 'https://www.mimuw.edu.pl/~mm319369/priv/d73890416bec03ff3e2b3756af8c941c/task1-data.zip'\n",
        "\n",
        "def download_zip(url):\n",
        "    from zipfile import ZipFile\n",
        "    try:\n",
        "        from urllib import urlretrieve\n",
        "    except:\n",
        "        from urllib.request import urlretrieve\n",
        "    from tempfile import mktemp\n",
        "\n",
        "    filename = mktemp('.zip')\n",
        "    name, hdrs = urlretrieve(data_url, filename)\n",
        "    thefile = ZipFile(filename)\n",
        "    thefile.extractall('')\n",
        "    thefile.close()\n",
        "    \n",
        "import os\n",
        "\n",
        "if not os.path.exists('taxi_custom.py'):\n",
        "    download_zip(data_url)\n",
        "    print(\"Done downloading\")\n",
        "else:\n",
        "    print(\"Files already exist\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done downloading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKiwjjWUZYiT"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from taxi_custom import TaxiBuilder\n",
        "\n",
        "TB = TaxiBuilder()\n",
        "SOLVE_SCORE_FOR_6 = 43"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-4jlnHdZd0p"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUwq6oemZYiX"
      },
      "source": [
        "class QPlayer(object):\n",
        "    def __init__(self, num_of_actions, num_of_states=None):\n",
        "        # Uzupełnij wartości parametrów\n",
        "        self.lr = 0.05 #learning rate: 0.0 - 1.0\n",
        "        self.gamma = 0.99 #gamma (discount factor): 0.0 - 1.0\n",
        "        self.epsilon_start = 1 #wartość początkowa dla epsilon-greedy: 0.0 - 1.0\n",
        "        self.epsilon_min = 0.01 #wartosc koncowa dla epsilon-greedy: 0.0 - 1.0\n",
        "        self.epsilon_decay = 0.99999 #szybkosc wygaszania epsilon: 0.9 - 0.999999...\n",
        "\n",
        "        self.num_of_actions = num_of_actions\n",
        "        self.num_of_states = num_of_states\n",
        "        \n",
        "        self.Q = defaultdict(lambda: np.zeros(self.num_of_actions))\n",
        "    \n",
        "    def encode_state(self, state):\n",
        "        # ta metoda moze byc wykorzystana, jezeli trzeba przetworzyć stan gry,\n",
        "        # np. jeżeli trzeba dokonać kwantyzacji \n",
        "        return state\n",
        "    \n",
        "    def train(self, env, steps):\n",
        "        # Metoda 'train' przyjmuje jako parametry zainicjowane srodowisko ai gym (env) oraz \n",
        "        # liczbę kroków uczenia (steps). Wewnątrz tej metody należy zaimplementować uczenie Q-learning\n",
        "        # Należy wykorzystac dobrane parametry uczenia (lr, gamma,...) zdefiniowane w __init__\n",
        "        \n",
        "        epsilon = self.epsilon_start\n",
        "        rewards = []\n",
        "        counter = 0\n",
        "\n",
        "        while steps > 0: \n",
        "          state = env.reset()\n",
        "          end = False\n",
        "          counter += 1\n",
        "          score = 0\n",
        "\n",
        "          while not end and steps > 0:\n",
        "\n",
        "            n = np.random.random(1)[0]\n",
        "\n",
        "            if n < epsilon:\n",
        "              action = env.action_space.sample() # losowa akcja \n",
        "            else:\n",
        "              action = np.argmax(self.Q[state]) # najlepsza akcja \n",
        "\n",
        "            new_state, reward, end, _ = env.step(action)\n",
        "\n",
        "            if end:\n",
        "              self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
        "            else:\n",
        "              self.Q[state][action] += self.lr * (reward + self.gamma * np.max(self.Q[new_state]) - self.Q[state][action])\n",
        "\n",
        "            if steps % 100_000 == 0:\n",
        "              if not rewards:\n",
        "                self.data = np.array([counter, steps, 0, epsilon])\n",
        "                print(counter, steps, \"\\t\", epsilon, sep='\\t')\n",
        "              else:\n",
        "                print(counter, steps, np.mean(rewards), epsilon, sep='\\t')\n",
        "                self.data = np.append(self.data, [counter, steps, np.mean(rewards), epsilon])\n",
        "                rewards = []\n",
        "\n",
        "            state = new_state\n",
        "            steps -= 1;\n",
        "            score += reward\n",
        "            if epsilon > self.epsilon_min:\n",
        "                epsilon *= self.epsilon_decay\n",
        "            \n",
        "        rewards.append(score)\n",
        "    \n",
        "    def play(self, state):\n",
        "        # Ta metoda zwraca (wyuczoną) akcje dla danego stanu\n",
        "        # Uzywana do 'grania' już wytrenowanym agentem\n",
        "        s = self.encode_state(state)\n",
        "        return np.argmax(self.Q[s])\n",
        "        \n",
        "        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS_dA64sZYib"
      },
      "source": [
        "## Rozwiązanie na planszy 6 – obowiązkowe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcGjWaREZYid",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539e7b8f-4725-430c-a945-7eb99dc032f9"
      },
      "source": [
        "# Bierzemy planszę o boku 6\n",
        "_, env = TB.get_instance(6)\n",
        "print(\"OBSERVATION SPACE:\", env.observation_space)\n",
        "print(\"ACTION SPACE:\", env.action_space)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OBSERVATION SPACE: Discrete(8192)\n",
            "ACTION SPACE: Discrete(6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkTtu5JqZYig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2121d0a-eab2-40b3-9429-fc1b201d939f"
      },
      "source": [
        "player = QPlayer(num_of_actions=env.action_space.n, num_of_states=env.observation_space.n)\n",
        "\n",
        "#Trenujemy\n",
        "player.train(env, 1_000_000)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t1000000\t\t\t1\n",
            "1694\t900000\t\t\t0.3678776017682482\n",
            "3735\t800000\t\t\t0.13533392988275578\n",
            "7519\t700000\t\t\t0.04978632156314083\n",
            "13164\t600000\t\t\t0.01831527257751119\n",
            "19033\t500000\t\t\t0.009999971601096055\n",
            "24928\t400000\t\t\t0.009999971601096055\n",
            "30824\t300000\t\t\t0.009999971601096055\n",
            "36732\t200000\t\t\t0.009999971601096055\n",
            "42612\t100000\t\t\t0.009999971601096055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmhyZHPaZYik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bebe323-73d5-41c4-c27e-97f319d5d165"
      },
      "source": [
        "def evaluate_player(player, env):\n",
        "     \n",
        "    scores = []\n",
        "    for i in range(1000):\n",
        "        score = 0\n",
        "        s = env.reset()\n",
        "        while(True):\n",
        "            a = player.play(s)\n",
        "            s, r, end, _ = env.step(a)\n",
        "            score += r\n",
        "            if end:\n",
        "                break\n",
        "                \n",
        "        #print(i, score)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return np.mean(scores)\n",
        "\n",
        "#Testowanie\n",
        "_, new_env = TB.get_instance(6)\n",
        "test_player = player\n",
        "\n",
        "score = evaluate_player(test_player, new_env)\n",
        "print(\"WYNIK:\", score)\n",
        "\n",
        "assert score >= SOLVE_SCORE_FOR_6, \"Trzeba jeszcze popracować...\"\n",
        "print(\"GRA ROZWIĄZANA!\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WYNIK: 43.948\n",
            "GRA ROZWIĄZANA!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq42HIgXZYin"
      },
      "source": [
        "## Uczenie na większych planszach\n",
        "Sprawdź, czy jesteśmy w stanie nauczyć agenta grać na planszach o większych rozmiarach. Jeśli nie, co stoi na przeszkodzie?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teA5jbqNZYio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de242c2-9849-4eab-e1e1-8b288f30c97b"
      },
      "source": [
        "_, env = TB.get_instance(10) # 6, 10, 20, 100\n",
        "print(\"OBSERVATION SPACE:\", env.observation_space)\n",
        "print(\"ACTION SPACE:\", env.action_space)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OBSERVATION SPACE: Discrete(8192)\n",
            "ACTION SPACE: Discrete(6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVkQm_GRZYiw"
      },
      "source": [
        "## Zdaj krótki raport w polu poniżej."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVzierZ-ZYix"
      },
      "source": [
        "Działa do plansz o boku do 10 w sensownym czasie.\n",
        "Potem przestrzeń stanów jest zbyt duża, by dojść do rozwiązania w sposób losowy."
      ]
    }
  ]
}