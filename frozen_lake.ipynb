{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDsD6_BOZ9lp"
      },
      "source": [
        "# FrozenLake 8x8\n",
        "\n",
        "Opis gry: https://gym.openai.com/envs/FrozenLake8x8-v0\n",
        "\n",
        "Celem gry we FrozenLake jest przejście z punktu początkowego (S) do końcowego (G). Gracz może się poruszać po polach oznaczonych jako (F). Wejście na pole (H) kończy grę. Po wybraniu akcji istnieje pewne prawdopodobieństwo wykonania innego ruchu niż wskazany.\n",
        "\n",
        "    \"SFFFFFFF\",\n",
        "    \"FFFFFFFF\",\n",
        "    \"FFFHFFFF\",\n",
        "    \"FFFFFHFF\",\n",
        "    \"FFFHFFFF\",\n",
        "    \"FHHFFFHF\",\n",
        "    \"FHFFHFHF\",\n",
        "    \"FFFHFFFG\"\n",
        "\n",
        "Zadanie:\n",
        "- zaimplementować Q-learning (w wersji niedeterministycznej)\n",
        "- dobrać parametry \n",
        "\n",
        "Opis stanu gry:\n",
        "- stan gry jest reprezentowany przez jedną liczbę\n",
        "\n",
        "Wskazówka: Dotarcie do stanu końcowego przy losowym wyborze akcji może wymagać bardzo wielu prób."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNZKK4LrZ_aY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71b2949-b771-49e5-a35e-777cadd45a5d"
      },
      "source": [
        "try:\n",
        "    import gym\n",
        "except:\n",
        "    !apt-get install cmake swig\n",
        "    !pip install gym[all]\n",
        "\n",
        "data_url = 'https://www.mimuw.edu.pl/~mm319369/priv/d73890416bec03ff3e2b3756af8c941c/task1-data.zip'\n",
        "\n",
        "def download_zip(url):\n",
        "    from zipfile import ZipFile\n",
        "    try:\n",
        "        from urllib import urlretrieve\n",
        "    except:\n",
        "        from urllib.request import urlretrieve\n",
        "    from tempfile import mktemp\n",
        "\n",
        "    filename = mktemp('.zip')\n",
        "    name, hdrs = urlretrieve(data_url, filename)\n",
        "    thefile = ZipFile(filename)\n",
        "    thefile.extractall('')\n",
        "    thefile.close()\n",
        "    \n",
        "import os\n",
        "\n",
        "if not os.path.exists('taxi_custom.py'):\n",
        "    download_zip(data_url)\n",
        "    print(\"Done downloading\")\n",
        "else:\n",
        "    print(\"Files already exist\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done downloading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcCWpl0FZ9lr"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "ENVIRONMENT = 'FrozenLake8x8-v0'\n",
        "SOLVE_SCORE = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sceSWkzZ9lw"
      },
      "source": [
        "class QPlayer(object):\n",
        "    def __init__(self, num_of_actions, num_of_states=None):\n",
        "        #Uzupełnij wartości parametrów\n",
        "        self.lr = 0.04 #learning rate: 0.0 - 1.0\n",
        "        self.gamma = 0.99 #gamma (discount factor): 0.0 - 1.0\n",
        "        self.epsilon_start = 1 #wartość początkowa dla epsilon-greedy: 0.0 - 1.0\n",
        "        self.epsilon_min = 0.01 #wartosc koncowa dla epsilon-greedy: 0.0 - 1.0\n",
        "        self.epsilon_decay = 0.999998 #szybkosc wygaszania epsilon: 0.9 - 0.999999...\n",
        "\n",
        "        self.num_of_actions = num_of_actions\n",
        "        self.num_of_states = num_of_states\n",
        "        \n",
        "        self.Q = defaultdict(lambda: np.zeros(self.num_of_actions))\n",
        "    \n",
        "    def encode_state(self, state):\n",
        "        return state\n",
        "    \n",
        "    def train(self, env, steps):\n",
        "        # Metoda 'train' przyjmuje jako parametry zainicjowane srodowisko ai gym (env) oraz \n",
        "        # liczbę kroków uczenia (steps). Wewnątrz tej metody należy zaimplementować uczenie Q-learning\n",
        "        # Należy wykorzystac dobrane parametry uczenia (lr, gamma,...) zdefiniowane w __init__\n",
        "        \n",
        "        epsilon = self.epsilon_start\n",
        "        rewards = []\n",
        "        counter = 0\n",
        "\n",
        "        while steps > 0: \n",
        "          state = env.reset()\n",
        "          end = False\n",
        "          counter += 1\n",
        "          score = 0\n",
        "\n",
        "        \n",
        "          while not end and steps > 0:\n",
        "\n",
        "            n = np.random.random(1)[0]\n",
        "\n",
        "            if n < epsilon:\n",
        "              action = env.action_space.sample() \n",
        "            else:\n",
        "              action = np.argmax(self.Q[state]) \n",
        "\n",
        "            new_state, reward, end, _ = env.step(action)\n",
        "\n",
        "            if end:\n",
        "              self.Q[state][action] = self.lr * reward + (1 - self.lr) * self.Q[state][action]\n",
        "            else:\n",
        "              self.Q[state][action] += self.lr * (reward + self.gamma * np.max(self.Q[new_state]) - self.Q[state][action])\n",
        "\n",
        "\n",
        "            if steps % 100000 == 0:\n",
        "              if not rewards:\n",
        "                self.data = np.array([counter, steps, 0, epsilon])\n",
        "                print(counter, steps, \"\\t\", epsilon, sep='\\t')\n",
        "              else:\n",
        "                print(counter, steps, np.mean(rewards), epsilon, sep='\\t')\n",
        "                self.data = np.append(self.data, [counter, steps, np.mean(rewards), epsilon])\n",
        "                rewards = []\n",
        "\n",
        "           \n",
        "            state = new_state\n",
        "            steps -= 1;\n",
        "            score += reward\n",
        "            if epsilon > self.epsilon_min:\n",
        "                epsilon *= self.epsilon_decay\n",
        "            \n",
        "        rewards.append(score)\n",
        "\n",
        "    \n",
        "    def play(self, state):\n",
        "        # Ta metoda zwraca (wyuczoną) akcje dla danego stanu\n",
        "        # Uzywana do 'grania' już wytrenowanym agentem\n",
        "        s = self.encode_state(state)\n",
        "        return np.argmax(self.Q[s])\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7dyKql3Z9lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01b1bfa3-53e2-4836-f07a-1d1532f3ddc7"
      },
      "source": [
        "env = gym.make(ENVIRONMENT)\n",
        "print(\"OBSERVATION SPACE:\", env.observation_space)\n",
        "print(\"ACTION SPACE:\", env.action_space)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OBSERVATION SPACE: Discrete(64)\n",
            "ACTION SPACE: Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1plpFSvrZ9l2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1033abd8-299b-4ff2-f20f-6635bddc7bc5"
      },
      "source": [
        "player = QPlayer(num_of_actions=env.action_space.n, num_of_states=env.observation_space.n)\n",
        "\n",
        "#Trenujemy\n",
        "iterations = 3500000\n",
        "player.train(env, iterations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\t3500000\t\t\t1\n",
            "2937\t3400000\t\t\t0.8187305893359925\n",
            "5329\t3300000\t\t\t0.6703197779144778\n",
            "7470\t3200000\t\t\t0.5488113068155073\n",
            "9310\t3100000\t\t\t0.44932860466333335\n",
            "10986\t3000000\t\t\t0.3678790733015422\n",
            "12580\t2900000\t\t\t0.30119385048855674\n",
            "14027\t2800000\t\t\t0.24659661871488162\n",
            "15439\t2700000\t\t\t0.2018961949687037\n",
            "16787\t2600000\t\t\t0.1652985906914241\n",
            "18059\t2500000\t\t\t0.1353350125732031\n",
            "19330\t2400000\t\t\t0.11080291460185661\n",
            "20595\t2300000\t\t\t0.09071773557212548\n",
            "21726\t2200000\t\t\t0.0742733851081955\n",
            "22969\t2100000\t\t\t0.0608098923616115\n",
            "24130\t2000000\t\t\t0.049786919010681524\n",
            "25296\t1900000\t\t\t0.04076207354283997\n",
            "26435\t1800000\t\t\t0.03337315649428631\n",
            "27537\t1700000\t\t\t0.02732362408457046\n",
            "28780\t1600000\t\t\t0.022370686849556468\n",
            "29955\t1500000\t\t\t0.018315565628188663\n",
            "31074\t1400000\t\t\t0.014995513840789482\n",
            "32138\t1300000\t\t\t0.012277285884265741\n",
            "33304\t1200000\t\t\t0.010051789507471586\n",
            "34508\t1100000\t\t\t0.009999995809393139\n",
            "35675\t1000000\t\t\t0.009999995809393139\n",
            "36834\t900000\t\t\t0.009999995809393139\n",
            "38019\t800000\t\t\t0.009999995809393139\n",
            "39076\t700000\t\t\t0.009999995809393139\n",
            "40228\t600000\t\t\t0.009999995809393139\n",
            "41367\t500000\t\t\t0.009999995809393139\n",
            "42456\t400000\t\t\t0.009999995809393139\n",
            "43547\t300000\t\t\t0.009999995809393139\n",
            "44636\t200000\t\t\t0.009999995809393139\n",
            "45799\t100000\t\t\t0.009999995809393139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BG6M9K_Z9l6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01185b39-2911-42cd-a560-72b2a810097d"
      },
      "source": [
        "def evaluate_player(player, env):\n",
        "     \n",
        "    scores = []\n",
        "    for i in range(10000):\n",
        "        score = 0\n",
        "        s = env.reset()\n",
        "        while(True):\n",
        "            a = player.play(s)\n",
        "            s, r, end, _ = env.step(a)\n",
        "            score += r\n",
        "            if end:\n",
        "                break\n",
        "                \n",
        "        #print(i, score)\n",
        "        scores.append(score)\n",
        "    \n",
        "    return np.mean(scores)\n",
        "\n",
        "#Testowanie\n",
        "new_env = gym.make(ENVIRONMENT)\n",
        "test_player = player\n",
        "\n",
        "score = evaluate_player(test_player, new_env)\n",
        "print(\"WYNIK:\", score)\n",
        "\n",
        "assert score >= SOLVE_SCORE, \"Trzeba jeszcze popracować...\"\n",
        "print(\"GRA ROZWIĄZANA!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WYNIK: 0.852\n",
            "GRA ROZWIĄZANA!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jgG1rYHZ9l9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}